
HOW DETERMINISM ACHIEVED IN RTOS...??





=======================================================================================================================

MUTEX vs SPINLOCK:

Mutex
 thread tries to lock a mutex and it does not succeed, because the mutex is already locked, it will go to sleep.
 immediately allowing another thread to run. 
 It will continue to sleep until being woken up, which will be the case once the mutex is being unlocked
 by whatever thread was holding the lock before.
 
Disadvantage: 
putting threads to sleep and waking them up again are both rather expensive operations.
they’ll need quite a lot of CPU instructions and thus also take some time.
If now the mutex was only locked for a very short amount of time, the time spent in putting a thread to sleep 
and waking it up again might exceed the time the thread has actually slept by far and it might even exceed 
the time the thread would have wasted by constantly polling on a spinlock

when to use Mutex??
when Lock is held for long time.

Spinlock: 
When a thread tries to lock a spinlock and it does not succeed, it will continuously re-try locking it,
until it finally succeeds; thus it will not allow another thread to take its place.
(however,once the CPU runtime quantum of the current thread has been exceeded, OS will forcefully switch to another thread,).

disadvantages:
polling on a spinlock will constantly waste CPU time.
if the lock is held for a longer amount of time, this will waste a lot more CPU time and it would have been much better 
if the thread was sleeping instead.

when to use spinlocaks??
Using spinlocks on a single-core/single-CPU system makes usually no sense, since as long as the spinlock polling is blocking
the only available CPU core, no other thread can run and since no other thread can run, the lock won’t be unlocked in the
quantum of time allocated for thread spinning...this merely wastes quantum of thread trying for spinlock.

On a multi-core/multi-CPU systems, with plenty of locks that are held for a very short amount of time only, 
the time wasted for constantly putting threads to sleep and waking them up again might decrease runtime performance noticeably. 
When using spinlocks instead, threads get the chance to take advantage of their full runtime quantum 
(always only blocking for a very short time period, but then immediately continue their work), leading to much higher processing throughput.

The Practice
Since very often programmers cannot know in advance if mutexes or spinlocks will be better 
(e.g. because the number of CPU cores of the target architecture is unknown), nor can operating systems know 
if a certain piece of code has been optimized for single-core or multi-core environments,
most systems don’t strictly distinguish between mutexes and spinlocks.
In fact, most modern operating systems have hybrid mutexes and hybrid spinlocks.

hybrid mutexes: 
behaves like a spinlock at first on a multi-core system.
Only if the lock has still not been obtained after a certain amount of time (or retries or any other measuring factor), 
the thread is really put to sleep.

===============================================================================================================================
Q : Difference Binary-semaphore and Mutex ??
A:

Binary semaphores and mutexes are very similar
subtle difference: 
                      Binary semaphores                                    mutexes

                    No priority inheritance mechanism                        Mutexes are binary semaphores that include
                                                                              a priority inheritance mechanism
                                                                              
                                                                              
                    Binary semaphore a better choice                          better choice for implementing
                    for implementing synchronisation                          simple mutual exclusion
                    (between tasks or between tasks and an interrupt) 
                    
                    semaphore can be released by any process                 only process acquired can relaese mutex
                       
Q: what is Mutex purpose??                       
Q:what is priority inheritance ? what is it's role in Mutex objective ?
  
   mutex : like a token to guard a resource.
   When a task wishes to access the resource it must first obtain the token.
   When it has finished with the resource it must 'give' the token back-allowing other tasks to access resource.
   
  scenario: Low-pri-taskA obtains mutex, high-pri-taskC which also require same mutex, got sceduled very next moment. 
  Result :  high-prio-taskC gets blocked for mutex and now chance that a medium-pri-taskB gets oppurtunity to run and finish.
            After taskB ,Low-pri-taskA gets chance to execute and release the mutex. high-pri-taskA gets chance to execute only
            after { Medium-pri-taskB Execution time + time taken by Low-pri-taskA to relese mutex} = Unbounded amount of time,untill
            Medium-pri-taskB finishes.
  
  Affect : Priority Inversion occured b/w Medim-pri-taskB and High-pri-taskC
  can we mitigate the Affect?? : priority inheritance: can't avoid priority inversion but can minimise the affect
  
  Role of priority Inheritance of Mutex - In minimising the Priority Inversion 
        if a high-pri-taskC blocks while attempting to obtain a mutex that is currently held by a 
        low-pri-taskA,then the priority of the taskA holding the mutex is temporarily raised to that of the blocking taskC
        ==> this doesn't allow Medim-pri-taskB to run.
        Now High-pri-taskC will run as soon as Low-pri-taskB releases the Lock( still priority Inversion , but minimised effect)
        
 Q: can we use Mutex in INTR-Handler ??
 A: Interrupt handler should not block waiting for some task to release mutex(if that task obtined mutex), 
    so we cant use mutex in INTR-Handler
    
    
 
Q: Re-entrant code ?? 

need :  single threaded process : -> single flow of control => code need not be re-entrant.
        Multi threaded process : ->  same Functions and resources accessed by several threads concurrently.
                                     re-entrant Fn : protects the resource integrity.
  
  
            
  Kernel-Memory Allocation:
  32bit ===> 2^32 = 4GB virtual Memory space.
  kernel is limited to 1GB-Virtual and Physical Memory
  The kernel's memory is not pageable. 
  The kernel usually wants physically contiguous memory
  Often, the kernel must allocate the memory without sleeping
  
  
  void * kmalloc(size_t size, int flags); ==>  allocates contiguous memory in physical memory as well as virtual memory
  flag :
  GFP_KERNEL => kmalloc can put current process in sleep state if memory is low.
                used in process context code when it is safe to sleep.
  GFP_ATOMIC:- ensures that current process is not put to sleep if memory is low.
               This is the flag to use in interrupt handlers, bottom halves and other situations where you cannot sleep.
  GFP_USER:-	This is a normal allocation and might block. This flag is used to allocate memory for user-space processes.


  kmalloc return the virtual address of first page allocated. On error it returns NULL.  
  
  when to use kmalloc:
  many hardware devices cannot address virtual memory. Therefore, in order for them to be able to access a block of memory,
  the block must exist as a physically contiguous chunk of memory
  Another benifit: a physically contiguous block of memory can use a single large page mapping.
                  This minimizes the translation lookaside buffer (TLB) overhead of addressing the memory
  
  
  void kfree(const void *ptr)
  calling kfree() on a block of memory that already has been freed or on a pointer that is not an address returned from kmalloc() 
  is a bug, and it can result in memory corruption. 
  
  Calling kfree() on NULL is checked for explicitly and is safe, although it is not necessarily a sensible idea
  
  
  When to use Kmalloc :-If memory required in driver needs to be contiguous in physical memory 
                        then we have to use kmalloc for allocation of memory.

void * vmalloc(undigned long size)
vmalloc allocates contigous memory in virtual memory but it doesn't guarantee that
        memory allocated in physical memory will be contiguous.
Upon successful allocation of memory it return virtual address of first byte of allocated memory block.
On failure it returns NULL.        
void *vfree(const void *ptr);

When to use vmalloc?
When you are sure that memory required will never needs to interact with hardware in future 
or we can if you required memory only for software purpose then we should use vmalloc for allocating memory.
If you are allocating memory that only software accesses, such as data associated with a user process, there is no need for the memory to be physically contiguous.


  Kmalloc or vmalloc ??                      
  it is often hard to find physically contiguous blocks of memory, especially for large allocations.
  Allocating memory that is only virtually contiguous has a much larger chance of success. 
  If you do not need physically contiguous memory, use vmalloc():  
  Nonetheless, few allocations in the kernel use vmalloc(). Most choose to use kmalloc(), even if it's not needed,
  partly for historical and partly for performance reasons. 
  Because the TLB overhead for physically contiguous pages is reduced greatly,
  the performance gains often are well appreciated. 
  Despite this, if you need to allocate tens of megabytes of memory in the kernel, vmalloc() is your best option. 
  
  Kmalloc                                                                           Vmalloc
  1)contiguous physical memory & contiguous virtual memory   2) not guarenteed contiguous physical memory but contiguous virtual memory 
  2)use for memory accessible by H/w                         2) for allocating buffers only for S/w usage
  3) performance is high : no TLB overhead                   3) TLB overhead : so dont use if high performance required while acessing mem allocated



Rules while Allocating memory in kernel:
Decide whether you can sleep (that is, whether the call to kmalloc() can block).
If you are in an interrupt handler, in a bottom half, or if you hold a lock, you cannot.
If you are in process context and do not hold a lock, you probably can. 
specify GFP_KERNEL. / specify GFP_ATOMIC accordingly
If you need DMA-capable memory : specify GFP_DMA. 
Always check for and handle a NULL return value from kmalloc()
Do not leak memory; make sure you call kfree() somewhere. 
Ensure you never access a block of memory after you free it. 





Kernel stack:
Unlike user-space processes, code executing in the kernel has neither a large nor a dynamically growing stack.
the kernel has a small fixed-size stack - 8KB typical  --- 4KB for recent versions
Never allocate large memory on kernel stack.


#define BUF_LEN	2048

void rabbit_function(void)
{
    char buf[BUF_LEN];
    /* ...  */
}

Instead, the following is preferred:

#define BUF_LEN	2048

void rabbit_function(void)
{
    char *buf;

    buf = kmalloc(BUF_LEN, GFP_KERNEL);
    if (!buf)
        /* error! */

/* ... */
}
In the kernel, however, you should use dynamic memory any time the allocation size is larger than a handful of bytes or so

Does each process have its own kernel stack ?
Not just each process - each thread has its own kernel stack (and, in fact, its own user stack as well). 
Remember the only difference between processes and threads (to Linux) is the fact that multiple threads
can share an address space (forming a process).

There is just one kernel memory. In it each process has
> > it's own task_struct + kernel stack (by default 8K). There  is no
special address mapping for these, nor are they allocated from a special area. 

The current design is :
the kernel stack space of process and its thread_info structure are located together (at two ends because stack grows down).
This is because it offers a key benefit in terms of efficiency: the kernel can easily obtain the address of the thread_info 
structure of the process currently running on a CPU from the value of its kernel ESP register.

task's kernel-stack to store a strcture thread_info
thread_info and task-struct contain pointers to each other


Switching b/w kernel stacks of processes:
When a context of some process is entered, esp is pointed to the top of
 it's stack. That's exactly all it takes to exchange stacks. 
  The scheduler saves most of the registers onto the stack,
then it saves the stack pointer in the task_struct of the
current thread and loads the stack pointer of the next
thread to be executed. Then it simply proceeds restoring
registers from the stack.

Since registers are saved both when a thread is interrupted (or enters kernel mode through a system call or an exception)
and also by the scheduler, there will actually be two sets of registers on the stack of each thread not currently
executing on a CPU.


OK, lets say there are 20 processes running in the system. Then the
> kernel must allocate 20 * 8K = 160K just for the stacks of these
> processes. All of these 160K always occupy the kernel (kernel memory
> is never swapped out). When a process actives, ESP would switch to
> point to the corresponding stack (of that process). 

Kernel-stack for Interrupts??
Depending on architecture and kernel version, the handler may
use either the kernel stack of the interrupted thread or a
special interrupt stack.

If a local variable is declared in an ISR, where it will be stored?
It will be stored in ISR stack(IRQSTACKSIZE). The ISR runs on a separate interrupt stack only if the hardware supports it. 
Otherwise, the ISR stack frames are pushed onto the stack of the interrupted thread.
As interrupts comes per cpu, therefore ISR stack has to be per cpu.



System calls:
glibc provides wrapper code which abstracts you away from the underlying code which arranges the arguments
you’ve passed and enters the kernel.

The Linux kernel includes a file which lists each system call in a table.
There are a few scripts which run at compile time which take syscalltable and generate the syscalls_32.h file from it.
this header file provides syscall numbers, which can be used by user programs.



The Linux kernel sets aside a specific software interrupt number that can be used by user space programs to enter the kernel
The Linux kernel registers an interrupt handler for the interrupt number: 128 (0x80)
The userland program is expected to put the system call number in the eax register.
The arguments for the syscall itself are to be placed in the remaining general purpose registers.
system-call service Function indexed using syscall number in system call table.

return address and the CPU flags which encode the privilege level (and other stuff),
and more are all saved on the program stack before ia32_syscall executes.

After system call service is Executed..kernel just needs to copy these values from the program stack back into the registers where they belong and execution will resume back in userland.
this is done using iret instruction.


Fast system calls:














  
  
               
               
  
  
   
   
   .
   
   
   
   
  
  


                       
                       
                       
                       
                       
================================================================================================================================                       
                       
                       
                       
                       
                       
