
HOW DETERMINISM ACHIEVED IN RTOS...??





=======================================================================================================================

MUTEX vs SPINLOCK:

Mutex
 thread tries to lock a mutex and it does not succeed, because the mutex is already locked, it will go to sleep.
 immediately allowing another thread to run. 
 It will continue to sleep until being woken up, which will be the case once the mutex is being unlocked
 by whatever thread was holding the lock before.
 
Disadvantage: 
putting threads to sleep and waking them up again are both rather expensive operations.
they’ll need quite a lot of CPU instructions and thus also take some time.
If now the mutex was only locked for a very short amount of time, the time spent in putting a thread to sleep 
and waking it up again might exceed the time the thread has actually slept by far and it might even exceed 
the time the thread would have wasted by constantly polling on a spinlock

when to use Mutex??
when Lock is held for long time.

Spinlock: 
When a thread tries to lock a spinlock and it does not succeed, it will continuously re-try locking it,
until it finally succeeds; thus it will not allow another thread to take its place.
(however,once the CPU runtime quantum of the current thread has been exceeded, OS will forcefully switch to another thread,).

disadvantages:
polling on a spinlock will constantly waste CPU time.
if the lock is held for a longer amount of time, this will waste a lot more CPU time and it would have been much better 
if the thread was sleeping instead.

when to use spinlocaks??
Using spinlocks on a single-core/single-CPU system makes usually no sense, since as long as the spinlock polling is blocking
the only available CPU core, no other thread can run and since no other thread can run, the lock won’t be unlocked in the
quantum of time allocated for thread spinning...this merely wastes quantum of thread trying for spinlock.

On a multi-core/multi-CPU systems, with plenty of locks that are held for a very short amount of time only, 
the time wasted for constantly putting threads to sleep and waking them up again might decrease runtime performance noticeably. 
When using spinlocks instead, threads get the chance to take advantage of their full runtime quantum 
(always only blocking for a very short time period, but then immediately continue their work), leading to much higher processing throughput.

The Practice
Since very often programmers cannot know in advance if mutexes or spinlocks will be better 
(e.g. because the number of CPU cores of the target architecture is unknown), nor can operating systems know 
if a certain piece of code has been optimized for single-core or multi-core environments,
most systems don’t strictly distinguish between mutexes and spinlocks.
In fact, most modern operating systems have hybrid mutexes and hybrid spinlocks.

hybrid mutexes: 
behaves like a spinlock at first on a multi-core system.
Only if the lock has still not been obtained after a certain amount of time (or retries or any other measuring factor), 
the thread is really put to sleep.

===============================================================================================================================
Q : Difference Binary-semaphore and Mutex ??
A:

Binary semaphores and mutexes are very similar
subtle difference: 
                      Binary semaphores                                    mutexes

                    No priority inheritance mechanism                        Mutexes are binary semaphores that include
                                                                              a priority inheritance mechanism
                                                                              
                                                                              
                    Binary semaphore a better choice                          better choice for implementing
                    for implementing synchronisation                          simple mutual exclusion
                    (between tasks or between tasks and an interrupt) 
                    
                    semaphore can be released by any process                 only process acquired can relaese mutex
                       
Q: what is Mutex purpose??                       
Q:what is priority inheritance ? what is it's role in Mutex objective ?
  
   mutex : like a token to guard a resource.
   When a task wishes to access the resource it must first obtain the token.
   When it has finished with the resource it must 'give' the token back-allowing other tasks to access resource.
   
  scenario: Low-pri-taskA obtains mutex, high-pri-taskC which also require same mutex, got sceduled very next moment. 
  Result :  high-prio-taskC gets blocked for mutex and now chance that a medium-pri-taskB gets oppurtunity to run and finish.
            After taskB ,Low-pri-taskA gets chance to execute and release the mutex. high-pri-taskA gets chance to execute only
            after { Medium-pri-taskB Execution time + time taken by Low-pri-taskA to relese mutex} = Unbounded amount of time,untill
            Medium-pri-taskB finishes.
  
  Affect : Priority Inversion occured b/w Medim-pri-taskB and High-pri-taskC
  can we mitigate the Affect?? : priority inheritance: can't avoid priority inversion but can minimise the affect
  
  Role of priority Inheritance of Mutex - In minimising the Priority Inversion 
        if a high-pri-taskC blocks while attempting to obtain a mutex that is currently held by a 
        low-pri-taskA,then the priority of the taskA holding the mutex is temporarily raised to that of the blocking taskC
        ==> this doesn't allow Medim-pri-taskB to run.
        Now High-pri-taskC will run as soon as Low-pri-taskB releases the Lock( still priority Inversion , but minimised effect)
        
 Q: can we use Mutex in INTR-Handler ??
 A: Interrupt handler should not block waiting for some task to release mutex(if that task obtined mutex), 
    so we cant use mutex in INTR-Handler
    
    
 
Q: Re-entrant code ?? 

need :  single threaded process : -> single flow of control => code need not be re-entrant.
        Multi threaded process : ->  same Functions and resources accessed by several threads concurrently.
                                     re-entrant Fn : protects the resource integrity.
  
  
            
  Kernel-Memory Allocation:
  32bit ===> 2^32 = 4GB virtual Memory space.
  kernel is limited to 1GB-Virtual and Physical Memory
  The kernel's memory is not pageable. 
  The kernel usually wants physically contiguous memory
  Often, the kernel must allocate the memory without sleeping
  
  
  void * kmalloc(size_t size, int flags); ==>  allocates contiguous memory in physical memory as well as virtual memory
  flag :
  GFP_KERNEL => kmalloc can put current process in sleep state if memory is low.
                used in process context code when it is safe to sleep.
  GFP_ATOMIC:- ensures that current process is not put to sleep if memory is low.
               This is the flag to use in interrupt handlers, bottom halves and other situations where you cannot sleep.
  GFP_USER:-	This is a normal allocation and might block. This flag is used to allocate memory for user-space processes.


  kmalloc return the virtual address of first page allocated. On error it returns NULL.  
  
  when to use kmalloc:
  many hardware devices cannot address virtual memory. Therefore, in order for them to be able to access a block of memory,
  the block must exist as a physically contiguous chunk of memory
  Another benifit: a physically contiguous block of memory can use a single large page mapping.
                  This minimizes the translation lookaside buffer (TLB) overhead of addressing the memory
  
  
  void kfree(const void *ptr)
  calling kfree() on a block of memory that already has been freed or on a pointer that is not an address returned from kmalloc() 
  is a bug, and it can result in memory corruption. 
  
  Calling kfree() on NULL is checked for explicitly and is safe, although it is not necessarily a sensible idea
  
  
  When to use Kmalloc :-If memory required in driver needs to be contiguous in physical memory 
                        then we have to use kmalloc for allocation of memory.

void * vmalloc(undigned long size)
vmalloc allocates contigous memory in virtual memory but it doesn't guarantee that
        memory allocated in physical memory will be contiguous.
Upon successful allocation of memory it return virtual address of first byte of allocated memory block.
On failure it returns NULL.        
void *vfree(const void *ptr);

When to use vmalloc?
When you are sure that memory required will never needs to interact with hardware in future 
or we can if you required memory only for software purpose then we should use vmalloc for allocating memory.
If you are allocating memory that only software accesses, such as data associated with a user process, there is no need for the memory to be physically contiguous.


  Kmalloc or vmalloc ??                      
  it is often hard to find physically contiguous blocks of memory, especially for large allocations.
  Allocating memory that is only virtually contiguous has a much larger chance of success. 
  If you do not need physically contiguous memory, use vmalloc():  
  Nonetheless, few allocations in the kernel use vmalloc(). Most choose to use kmalloc(), even if it's not needed,
  partly for historical and partly for performance reasons. 
  Because the TLB overhead for physically contiguous pages is reduced greatly,
  the performance gains often are well appreciated. 
  Despite this, if you need to allocate tens of megabytes of memory in the kernel, vmalloc() is your best option. 
  
  Kmalloc                                                                           Vmalloc
  1)contiguous physical memory & contiguous virtual memory   2) not guarenteed contiguous physical memory but contiguous virtual memory 
  2)use for memory accessible by H/w                         2) for allocating buffers only for S/w usage
  3) performance is high : no TLB overhead                   3) TLB overhead : so dont use if high performance required while acessing mem allocated



Rules while Allocating memory in kernel:
Decide whether you can sleep (that is, whether the call to kmalloc() can block).
If you are in an interrupt handler, in a bottom half, or if you hold a lock, you cannot.
If you are in process context and do not hold a lock, you probably can. 
specify GFP_KERNEL. / specify GFP_ATOMIC accordingly
If you need DMA-capable memory : specify GFP_DMA. 
Always check for and handle a NULL return value from kmalloc()
Do not leak memory; make sure you call kfree() somewhere. 
Ensure you never access a block of memory after you free it. 





Kernel stack:
Unlike user-space processes, code executing in the kernel has neither a large nor a dynamically growing stack.
the kernel has a small fixed-size stack - 8KB typical  --- 4KB for recent versions
Never allocate large memory on kernel stack.


#define BUF_LEN	2048

void rabbit_function(void)
{
    char buf[BUF_LEN];
    /* ...  */
}

Instead, the following is preferred:

#define BUF_LEN	2048

void rabbit_function(void)
{
    char *buf;

    buf = kmalloc(BUF_LEN, GFP_KERNEL);
    if (!buf)
        /* error! */

/* ... */
}
In the kernel, however, you should use dynamic memory any time the allocation size is larger than a handful of bytes or so

Does each process have its own kernel stack ?
Not just each process - each thread has its own kernel stack (and, in fact, its own user stack as well). 
Remember the only difference between processes and threads (to Linux) is the fact that multiple threads
can share an address space (forming a process).

There is just one kernel memory. In it each process has
> > it's own task_struct + kernel stack (by default 8K). There  is no
special address mapping for these, nor are they allocated from a special area. 

The current design is :
the kernel stack space of process and its thread_info structure are located together (at two ends because stack grows down).
This is because it offers a key benefit in terms of efficiency: the kernel can easily obtain the address of the thread_info 
structure of the process currently running on a CPU from the value of its kernel ESP register.

task's kernel-stack to store a strcture thread_info
thread_info and task-struct contain pointers to each other


Switching b/w kernel stacks of processes:
When a context of some process is entered, esp is pointed to the top of
 it's stack. That's exactly all it takes to exchange stacks. 
  The scheduler saves most of the registers onto the stack,
then it saves the stack pointer in the task_struct of the
current thread and loads the stack pointer of the next
thread to be executed. Then it simply proceeds restoring
registers from the stack.

Since registers are saved both when a thread is interrupted (or enters kernel mode through a system call or an exception)
and also by the scheduler, there will actually be two sets of registers on the stack of each thread not currently
executing on a CPU.


OK, lets say there are 20 processes running in the system. Then the
> kernel must allocate 20 * 8K = 160K just for the stacks of these
> processes. All of these 160K always occupy the kernel (kernel memory
> is never swapped out). When a process actives, ESP would switch to
> point to the corresponding stack (of that process). 

Kernel-stack for Interrupts??
Depending on architecture and kernel version, the handler may
use either the kernel stack of the interrupted thread or a
special interrupt stack.

If a local variable is declared in an ISR, where it will be stored?
It will be stored in ISR stack(IRQSTACKSIZE). The ISR runs on a separate interrupt stack only if the hardware supports it. 
Otherwise, the ISR stack frames are pushed onto the stack of the interrupted thread.
As interrupts comes per cpu, therefore ISR stack has to be per cpu.



System calls:
glibc provides wrapper code which abstracts you away from the underlying code which arranges the arguments
you’ve passed and enters the kernel.

The Linux kernel includes a file which lists each system call in a table.
There are a few scripts which run at compile time which take syscalltable and generate the syscalls_32.h file from it.
this header file provides syscall numbers, which can be used by user programs.



The Linux kernel sets aside a specific software interrupt number that can be used by user space programs to enter the kernel
The Linux kernel registers an interrupt handler for the interrupt number: 128 (0x80)
The userland program is expected to put the system call number in the eax register.
The arguments for the syscall itself are to be placed in the remaining general purpose registers.
system-call service Function indexed using syscall number in system call table.

return address and the CPU flags which encode the privilege level (and other stuff),
and more are all saved on the program stack before ia32_syscall executes.

After system call service is Executed..kernel just needs to copy these values from the program stack back into the registers where they belong and execution will resume back in userland.
this is done using iret instruction.


Fast system calls:










SCHEDULER:

From a high level, the scheduler is simply a grouping of functions that operate on given data structures.--kernel/sched.c 
A task/process/thread in the scheduler is a collection of data structures and flow of control.


a process has been initialized and placed on a run queue:

struct task_struct *p;

p->state = TASK_RUNNING ; =====> task is in run-Q

In Linux, the run queue is composed of two priority arrays:

    Active.: Stores processes that have not yet used up their timeslice

    Expired.: Stores processes that have used up their timeslice.

From a high level, the scheduler’s job in Linux is to take the highest priority active processes, 
let them use the CPU to execute, 
and place them in the expired array when they use up their timeslice.

 at some time, this process should have access to the CPU to execute.
 The two functions that are responsible for passing CPU control to different processes are schedule() and scheduler_tick()
schedule(): how the Linux kernel decides which process to execute next
scheduler_tick() : how the kernel determines which processes need to yield the CPU ??

When a timer event occurs,the current process is put on hold and the Linux kernel itself takes control of the CPU.
scheduler_tick() is a system timer isr that the kernel calls and marks processes as needing rescheduling / not needed.
When the timer event finishes, the Linux kernel normally passes control back to the process that was put on hold. However, when the held process has been marked as needing rescheduling, the kernel calls schedule() to choose which process to activate instead of the process that was executing before the kernel took control. The process that was executing before the kernel took control is called the current process.

Example:

Process A has control of the CPU and is executing.

The system timer scheduler_tick() goes off, takes control of the CPU from A, and marks A as needing rescheduling.
The Linux kernel calls schedule(), which chooses Process B and the control of the CPU is given to B.

Process B executes for a while and then voluntarily yields the CPU.
This commonly occurs when a process waits on some resource.
B calls schedule(), which chooses Process C to execute next.

Process C executes until scheduler_tick() occurs, which does not mark C as needing rescheduling.
This results in schedule() not being called and C regains control of the CPU.

The system timer scheduler_tick() goes off, takes control of the CPU from C, and marks C as needing rescheduling
The Linux kernel calls schedule(), which chooses Process A and the control of the CPU is given to A.


PROCESS states in KERNEL-SPACE:
----------------------------------

p->state = RUNNING : run-Q => ready state==> ready to run in run-Q / running on cpu.
yielding the processor:
calling schedule();==> when p->state = RUNNING,merely moves process to run-Q. this will execute again when scheduler chooses it.

sceduler can't choose to execute a process , if it is sleeping / waiting on an Event.:
SLEEPING in the kernel:
If procesing is waiting on some resource , It blocks by calling schedule().
set_current_state(TASK_INTERRUPTIBLE); or set_current_state(TASK_UNINTERRUPTIBLE);
schedule();
Interruptible and Uninterruptible Sleep:
Interruptible sleep is the preferred way of sleeping, unless there is a situation in which signals cannot be handled at all,
such as device I/O. 
p-> state = TASK_INTERRUPTIBLE  : can be wokenup by signals or by calling wake_up_process()from other process kernel code.
p-> state = TASK_UNINTERRUPTIBLE : can be wokenup only by wake_up_process() call.

########################################################################################################


SLEEPING in Kernel -Detailed Explaination:
--------------------------------------------
Typical scenario for blocking on event / sleeping:

A --> processing list items 
B---> adding items to list
To access-List : lock should be aquired.

Process A:
1  spin_lock(&list_lock);               
2  if(list_empty(&list_head)) {
3      spin_unlock(&list_lock);
4      set_current_state(TASK_INTERRUPTIBLE);
5      schedule();
6      spin_lock(&list_lock);
7  }
8
9  /* Rest of the code ... */
10 spin_unlock(&list_lock);
==========>
LOCK , listEmpty-Check
if Empty - UNLOCK , SLEEP .
when wokeup by B --> LOCK , process list-item , UNLOCK
here A-> is marked as RUNNING by wake_up_process call of B.
and is sceduled only when A get it's time slot allocated.



Process B:
100  spin_lock(&list_lock);
101  list_add_tail(&list_head, new_node);
102  spin_unlock(&list_lock);
103  wake_up_process(processa_task);

LOCK ,Add-listItem, UNLOCK, WAKEUP--> marks A - RUNNING., do other work, time-slotExp-> relinquish CPU 


Lost Wake-Up Problem:
------------------------
Race condition: After  LOCK and before SLEEP if processA time-slot Expires.
                process-B scheduled , adds item, and Wakesup-A, does some other work,after-timeslot expires moved out of cpu.
                process-A sceduled , SLEEPs...woken up only when B-sceduled Again,Adds-listItem,wakes-up A.
                processA --lost one Wakeup : common problem in kernel.

solution:
-----------
Process A:

1  set_current_state(TASK_INTERRUPTIBLE);
2  spin_lock(&list_lock);
3  if(list_empty(&list_head)) {
4         spin_unlock(&list_lock);
5         schedule();
6         spin_lock(&list_lock);
7  }
8  set_current_state(TASK_RUNNING);
9
10 /* Rest of the code ... */
11 spin_unlock(&list_lock);
====> 
A----> MARK as INTERRUPTIBLE , LOCK, check-ListEmpty, if Empty UNLOCK and yield CPU.

if processA -loses cpu before schedule()---> process B adds Item and wake_up_process(A)- marks A as RUNNING.,
B executed for some time and loses cpu.
now  when A  runs -->schedule() call merily causes A to yields CPU.==> A wont sleep.
hence..when A gets chance to run based on scheduler Algorithm , it processes ListItem.



Example code in kernel: 


4253  /* Wait for kthread_stop */
4254  set_current_state(TASK_INTERRUPTIBLE);
4255  while (!kthread_should_stop()) {
4256          schedule();
4257          set_current_state(TASK_INTERRUPTIBLE);
4258  }
4259  __set_current_state(TASK_RUNNING);
4260 return 0;

The thread cannot exit until the kthread_should_stop() function return TRUE.
while waiting for the function to return 0 : thread sleeps 
it keeps checking untill the Fn to return TRUE.,,whenever woken-up by other thread.
kthread_should_stop condition is made only after the state is TASK_INTERRUPTIBLE. 
Hence, the wake-up received after the condition check but before the call to schedule() function is not lost. 


Wait queues:  higher-level mechanism used to put processes to sleep and wake them up

wait_queue_head_t my_event;
init_waitqueue_head(&my_event);
(or)
DECLARE_WAIT_QUEUE_HEAD(my_event);

Wait:
     wait_event(&my_event, (event_present == 1) );   -------------------->  UNINTERRUPTIBLE sleep
    wait_event_interruptible(&my_event, (event_present == 1) ); --------->  INTERRUPTIBLE sleep

Wakeup:    
    wake_up(&my_event);: wakes up only one process from the wait queue.
    wake_up_all(&my_event);: wakes up all the processes on the wait queue.
    wake_up_interruptible(&my_event);: wakes up only one process from the wait queue that is in interruptible sleep. 
    
Example:
291 static int smbiod(void *unused)
292 {
299     for (;;) {
300             struct smb_sb_info *server;
301             struct list_head *pos, *n;
302
303             /* FIXME: Use poll? */
304             wait_event_interruptible(smbiod_wait,
305                     test_bit(SMBIOD_DATA_READY,
                                 &smbiod_flags));
...
...             /* Some processing */
312
313             clear_bit(SMBIOD_DATA_READY,
                          &smbiod_flags);
                     
...             /* Code to perform the requested I/O */
...
...
337     }
338
339     VERBOSE("SMB Kernel thread exiting (%d)...\n", current->pid);
340     module_put_and_exit(0);
341 }
342


Thundering Herd Problem:  arises due to the use of the wake_up_all function

scenario :  set of processes are sleeping on a wait queue, wanting to acquire a lock.
            Once the process that has acquired the lock is done with it,
            it releases the lock and wakes up all the processes sleeping on the wait queue.
            All the processes try to grab the lock. Eventually, only one of these acquires the lock
            and the rest go back to sleep.
penalty : It consumes valuable CPU cycles and incurs context-switching overheads

Q: If we already know that only one process is going to resume while the rest of the processes go back to sleep again,
   why wake them up in the first place?
A: wake_up_all function should be done carefully, only when you know that it is required.
Otherwise, go ahead and use the wake_up function that wakes up only one process at a time. 

Q: when would the wake_up_all function be used? 
A: It is used in scenarios when processes want to take a shared lock on something. 
  For example, processes waiting to read data on a page could all be woken up at the same moment. 
  
Time-Bound Sleep":
schedule_timeout(timeout), a variant of schedule():  puts the process to sleep until timeout jiffies have elapsed.
set_current_state(TASK_INTERRUPTIBLE);
schedule_timeout(20msec);

If execution of your process to be delayed for a given amount of time. 
It may be required to allow the hardware to catch up 
or to carry out an activity after specified time intervals, such as polling a device, flushing data to disk or retransmitting a network request.

1415  set_current_state(TASK_INTERRUPTIBLE);
1416  for (;;) {
1417     schedule_timeout(APM_CHECK_TIMEOUT);
1418     if (exit_kapmd)
1419         break;
1421      * Ok, check all events, check for idle
....      * (and mark us sleeping so as not to
....      * count towards the load average)..
1423      */
1424      set_current_state(TASK_INTERRUPTIBLE);
1425      apm_event_handler();
1426  }

 You also may use a more convenient API, with which you can specify time in milliseconds and seconds:
    msleep(time_in_msec);
    msleep_interruptible(time_in_msec);
    ssleep(time_in_sec); 
These higher-level routines internally convert the time into jiffies,
appropriately change the state of the process and call schedule_timeout(), thus making the process sleep.     


    
                   
                   
                   
  

   
PREEMPTION in linux:   
   
Until kernel version 2.4, only user processes were preemptive, 
i.e., in addition to time quantum expiration, an execution of current process in user mode would be interrupted 
if higher dynamic priority processes entered TASK_RUNNING state .

Toward 2.6 series of the Linux kernel, an ability to interrupt a task executing kernel code was added, 
although with that not all sections of the kernel code can be preempted.

The Linux kernel provides preemptive scheduling under certain conditions.

Through the use of the real-time Linux kernel patch PREEMPT_RT, 
support for full preemption of critical sections, interrupt handlers, and "interrupt disable" code sequences can be supported.

Preemption improves latency, increases responsiveness, and makes Linux more suitable for desktop and real-time applications

Critical sections protected by semaphores can be preempted at any time, 
as every contention will end in a schedule and there can't be any deadlock.

However, critical sections protected by fast spin locks or by hand locks cannot be preempted unless we block the timer IRQ. 
So all spin locks should be IRQ-safe.



Preemption 
----------------

On UP, spin_lock is defined as preempt_disable, and spin_unlock is defined as preempt_enable.
On SMP, they also perform locking.

The nestable preemption markers(preempt_disable and preempt_enable) operate on preempt_count,
 a new integer stored in each task_struct.

preempt_disable effectively is:

++current->preempt_count;
barrier();


and preempt_enable is:

--current->preempt_count;
barrier();
if (unlikey(!current->preempt_count
    && current->need_resched))
    preempt_schedule();



The result is we do not preempt when the count is greater than zero. 

preempt_schedule:  It sets a flag in the current process to signify it was preempted,
calls schedule and, upon return, unsets the flag:

asmlinkage void preempt_schedule(void)
{
    do {
        current->preempt_count += PREEMPT_ACTIVE;
        schedule();
        current->preempt_count -= PREEMPT_ACTIVE;
    } while (current->need_resched);
}



Implicitly locked data vs pre-emption
-----------------------------------------
In non-preemptive kernel:
per-CPU data (data structures unique to each CPU) do not require locking. 
such data is protected by its nature, is considered to be “implicitly locked”.
a task on another CPU cannot mangle the first CPU's data. 

But Implicitly locked data and preemption do not get along.
With preemption, a process on the same CPU can find itself preempted,
and a second process can then trample on the data of the first.

such data can be  protected by the existing SMP locks.

is there any other way to protect such a data without using SMP-locks??

The solution, thankfully, is simple: disable preemption around access to the data. 

For example:

int catface[NR_CPUS];
preempt_disable();
catface[smp_processor_id()] = 1;  /* index catface
                                     by CPU number */
/* operate on catface */
preempt_enable();






















                
                
                
                














  
  
               
               
  
  
   
   
   .
   
   
   
   
  
  


 
